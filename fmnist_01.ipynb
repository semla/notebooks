{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fmnist-01.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/semla/notebooks/blob/master/fmnist_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "VtEcg8p3gR3_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install dependencies & import\n",
        "Install Keras and Tensorflow using pip."
      ]
    },
    {
      "metadata": {
        "id": "rjxoiYwLfV3A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U keras tensorflow>=1.8.0 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1LEzzRPjrHzJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import statemets"
      ]
    },
    {
      "metadata": {
        "id": "DRk8v2zUgLgY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.models import Sequential # For sequential models, not functional\n",
        "from keras.layers import Dense, Activation,MaxPooling2D, Dropout, Conv2D, Flatten # the layers used\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils import to_categorical # one-hot encoding\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf # tensorflow is the backend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XXk-QeSa7M25",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set variables to enable / disable features depending on where the code is run"
      ]
    },
    {
      "metadata": {
        "id": "Lq37z-om7YhU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c5599cb-de38-41db-c3fd-62d151f84585"
      },
      "cell_type": "code",
      "source": [
        "use_matplotlib, use_google_drive, use_gpu = True, True, True\n",
        "\n",
        "# use gpu\n",
        "if use_gpu:\n",
        "  device_name = tf.test.gpu_device_name()\n",
        "  if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "  print('Using GPU at: {}'.format(device_name))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tx_0GeqAsGXu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load data & do some preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "AZ4Y7HLoixEV",
        "colab_type": "code",
        "outputId": "a061b490-3c62-43f8-b462-062dc63e983d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the fashion-mnist pre-shuffled train data and test data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Print training set shape - note there are 60,000 training data of image size of 28x28, 60,000 train labels)\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "\n",
        "# Print the number of training and test datasets\n",
        "print(x_train.shape[0], 'train set')\n",
        "print(x_test.shape[0], 'test set')\n",
        "\n",
        "# Define the text labels\n",
        "fashion_mnist_labels = [\"T-shirt/top\",  # index 0\n",
        "                        \"Trouser\",      # index 1\n",
        "                        \"Pullover\",     # index 2 \n",
        "                        \"Dress\",        # index 3 \n",
        "                        \"Coat\",         # index 4\n",
        "                        \"Sandal\",       # index 5\n",
        "                        \"Shirt\",        # index 6 \n",
        "                        \"Sneaker\",      # index 7 \n",
        "                        \"Bag\",          # index 8 \n",
        "                        \"Ankle boot\"]   # index 9\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28) y_train shape: (60000,)\n",
            "60000 train set\n",
            "10000 test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FFpw7yorqsWJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Normalization  & add a dimension for channel. Even if it is black & white images Keras wants a number for channels."
      ]
    },
    {
      "metadata": {
        "id": "XLXfDHz0h2Oj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# the preprocess_input wants samples, x-values, y-values and channels\n",
        "# since black & white only one channel, so add a 1\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# \"One-hot-encode\" the labels in train and test\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y0P_eWjrCvTp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Further break training data into training and validation sets, 5000 in validation set and remaining 55,000 for training set)\n",
        "(x_train, x_valid) = x_train[5000:], x_train[:5000] \n",
        "(y_train, y_valid) = y_train[5000:], y_train[:5000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3cUbVvfWIwDW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Explore the data.\n",
        "Take a look at the y_train which, is now one-hot-encoded.\n",
        "Print the matrix of pixel values representing an image, then print the same data as an image."
      ]
    },
    {
      "metadata": {
        "id": "mkLwP3amItip",
        "colab_type": "code",
        "outputId": "2f413f36-544f-4645-e27c-6ef8260d9e0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        }
      },
      "cell_type": "code",
      "source": [
        "# Choose one of the 60000 training samples\n",
        "img_index = 2\n",
        "# y_train contains the lables, ranging from 0 to 9\n",
        "print(\"Row number \" + str(img_index) +\" of the labels: \" + str(y_train[img_index]))\n",
        "label_index=np.argmax(y_train[img_index])\n",
        "# Print the label, for example 2 Pullover\n",
        "print (\"Category \"+str(label_index+1) + \" is \" +(fashion_mnist_labels[label_index]))\n",
        "\n",
        "# Show one of the images from the training dataset\n",
        "# reshaping neccessary to only show the columns with actual pixel data (not the one added with reshape above)\n",
        "img=x_train[img_index].reshape(28,28)\n",
        "np.set_printoptions(linewidth=125)\n",
        "print(\"The matrix of pixel values for sample \" + str(img_index) + \":\\n\" + str(np.around(img,1)))\n",
        "\n",
        "# show the same data but as an image\n",
        "if use_matplotlib:\n",
        "  plt.imshow(img)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Row number 2 of the labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Category 8 is Sneaker\n",
            "The matrix of pixel values for sample 2:\n",
            "[[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.5 0.4 0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5 0.5 0.6 0.8 0.7 0.6 0.3 0.1 0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.6 1.  0.6 0.6 0.9 0.7 0.7 0.6 0.7 0.5 0.2 0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.6 0.4 0.9 0.8 0.7 0.6 0.5 0.5 0.6 0.7 0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.3 0.6 0.9 0.3 0.5 0.8 0.8 0.8 0.4 0.4 0.4 0.5 0.5 0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.1 0.6 0.8 0.8 0.7 0.8 0.6 0.6 0.6 0.5 0.5 0.5 0.6 0.1]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.1 0.  0.4 0.7 0.4 0.2 0.6 0.8 0.8 0.6 0.4 0.5 0.5 0.5 0.5 0.9 0.2]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.1 0.3 0.5 0.  0.4 0.5 0.6 0.9 0.6 0.6 0.6 0.4 0.3 0.3 0.4 0.4 0.7 0.1]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.2 0.4 0.  0.7 0.4 0.3 0.5 0.8 0.7 0.6 0.5 0.6 0.7 0.6 0.5 0.4 0.4 0.6 0.2]\n",
            " [0.  0.  0.1 0.2 0.3 0.3 0.3 0.4 0.1 0.5 0.9 0.1 0.2 0.5 0.8 0.9 0.7 0.5 0.5 0.5 0.5 0.6 0.6 0.5 0.6 0.5 0.6 0.3]\n",
            " [0.  0.1 0.3 0.3 0.2 0.1 0.1 0.3 0.3 0.3 0.3 0.5 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.5 0.4 0.5 0.5 0.5 0.6 0.5 0.6 0.3]\n",
            " [0.  0.1 0.  0.1 0.1 0.1 0.4 0.4 0.3 0.5 0.5 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.5 0.5 0.5 0.5 0.7 0.6 0.6 0.7 0.4]\n",
            " [0.  0.1 0.1 0.1 0.1 0.1 0.2 0.3 0.4 0.5 0.5 0.5 0.5 0.6 0.7 0.7 0.6 0.6 0.6 0.5 0.4 0.4 0.6 0.7 0.6 0.5 0.5 0.3]\n",
            " [0.3 0.2 0.  0.1 0.2 0.1 0.1 0.1 0.  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.2 0.2 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.6 0.5 0.4 0.2 0.1 0.1 0.1 0.2 0.2 0.1 0.2 0.1 0.1 0.2 0.2 0.1 0.1 0.2 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.  0.  0.1 0.4 0.7 0.7 0.6 0.6 0.3 0.3 0.3 0.2 0.2 0.3 0.2 0.3 0.3 0.3 0.3 0.3 0.4 0.4 0.3 0.3 0.3 0.3 0.4 0.1]\n",
            " [0.  0.  0.  0.  0.  0.  0.2 0.4 0.4 0.5 0.6 0.5 0.5 0.6 0.1 0.2 0.2 0.2 0.1 0.1 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe572f764a8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "yT0wWpewrZg4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The model\n",
        "##Create the model and add layers"
      ]
    },
    {
      "metadata": {
        "id": "nxUpYlskrcPM",
        "colab_type": "code",
        "outputId": "cc94d243-3dab-4d7e-850b-0386da457262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "cell_type": "code",
      "source": [
        "my_model = Sequential()\n",
        "\n",
        "my_model.add( Conv2D(filters=64, kernel_size=(4,4), padding='same', activation='relu', input_shape=(28,28,1))) \n",
        "my_model.add( MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "my_model.add( Dropout(rate=0.3))\n",
        "\n",
        "my_model.add( Conv2D(filters=64, kernel_size=(4,4), padding='same', activation='relu'))\n",
        "my_model.add( MaxPooling2D(pool_size=2))\n",
        "my_model.add( Dropout(0.3))\n",
        "\n",
        "my_model.add( Flatten())\n",
        "my_model.add( Dense(256, activation='relu'))\n",
        "#my_model.add( Dropout(0.5))\n",
        "my_model.add( Dense(10, activation='softmax'))\n",
        "\n",
        "# print the number of layers\n",
        "print('Number of layers: ' + str(len(my_model.layers)))\n",
        "\n",
        "# print sumary of the model and its layers\n",
        "my_model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of layers: 9\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 64)        1088      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 64)        65600     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               803072    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 872,330\n",
            "Trainable params: 872,330\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vQUSC9dNLH7S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Compile the model"
      ]
    },
    {
      "metadata": {
        "id": "vkGAQGRsKaEn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             #optimizer='SGD',\n",
        "             #optimizer='RMSProp',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s6bIaeBaPnD4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "Save training to history"
      ]
    },
    {
      "metadata": {
        "id": "vJhPxCzCS-X4",
        "colab_type": "code",
        "outputId": "af722a13-fa06-438d-f746-7797613380a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpointer = ModelCheckpoint(filepath='best.hdf5', verbose = 1, save_best_only=True)\n",
        "bs = 64\n",
        "ep = 10\n",
        "\n",
        "history = my_model.fit(x_train,\n",
        "         y_train,\n",
        "         batch_size=bs,\n",
        "         epochs=ep,\n",
        "         validation_data=(x_valid, y_valid),\n",
        "         callbacks=[checkpointer])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 55000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "55000/55000 [==============================] - 15s 275us/step - loss: 0.4649 - acc: 0.8327 - val_loss: 0.2991 - val_acc: 0.8932\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.29911, saving model to best.hdf5\n",
            "Epoch 2/10\n",
            "55000/55000 [==============================] - 12s 222us/step - loss: 0.3075 - acc: 0.8881 - val_loss: 0.2592 - val_acc: 0.9048\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.29911 to 0.25919, saving model to best.hdf5\n",
            "Epoch 3/10\n",
            "55000/55000 [==============================] - 12s 222us/step - loss: 0.2639 - acc: 0.9035 - val_loss: 0.2333 - val_acc: 0.9148\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.25919 to 0.23331, saving model to best.hdf5\n",
            "Epoch 4/10\n",
            "55000/55000 [==============================] - 12s 223us/step - loss: 0.2397 - acc: 0.9102 - val_loss: 0.2221 - val_acc: 0.9172\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.23331 to 0.22207, saving model to best.hdf5\n",
            "Epoch 5/10\n",
            "55000/55000 [==============================] - 12s 222us/step - loss: 0.2183 - acc: 0.9177 - val_loss: 0.2083 - val_acc: 0.9210\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.22207 to 0.20832, saving model to best.hdf5\n",
            "Epoch 6/10\n",
            "55000/55000 [==============================] - 12s 221us/step - loss: 0.2002 - acc: 0.9243 - val_loss: 0.2146 - val_acc: 0.9208\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.20832\n",
            "Epoch 7/10\n",
            "55000/55000 [==============================] - 12s 222us/step - loss: 0.1850 - acc: 0.9300 - val_loss: 0.1984 - val_acc: 0.9264\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.20832 to 0.19838, saving model to best.hdf5\n",
            "Epoch 8/10\n",
            "55000/55000 [==============================] - 12s 220us/step - loss: 0.1755 - acc: 0.9337 - val_loss: 0.2028 - val_acc: 0.9246\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.19838\n",
            "Epoch 9/10\n",
            "55000/55000 [==============================] - 12s 221us/step - loss: 0.1597 - acc: 0.9394 - val_loss: 0.2062 - val_acc: 0.9296\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.19838\n",
            "Epoch 10/10\n",
            "55000/55000 [==============================] - 12s 221us/step - loss: 0.1486 - acc: 0.9424 - val_loss: 0.1993 - val_acc: 0.9282\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.19838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BRPq4Rn_ks42",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Plot feature map"
      ]
    },
    {
      "metadata": {
        "id": "TE95EpHciyRM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "mount google drive"
      ]
    },
    {
      "metadata": {
        "id": "efcr2EM6i5GY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if use_google_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vSzqfc9-TEAM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Plot success"
      ]
    },
    {
      "metadata": {
        "id": "k1e7z2BQPrDw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(type(history.history))\n",
        "print(len(history.history))\n",
        "print(len(history.history['loss']))\n",
        "# print(history.history)\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy, epochs: ' + str(ep) + ', batch size: ' + str(bs))\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "# plt.axis([0, len(history.history['loss']),0,1])\n",
        "plt.legend(['Train', 'Validaton'], loc='upper left')\n",
        "save_png('acc',ep,bs)\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss, epochs: '+ str(ep) + ', batch size: ' + str(bs))\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "#plt.axis([0, len(history.history['loss']),0,1])\n",
        "plt.axis\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a48C1zmx7qcL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function to save image to google drive"
      ]
    },
    {
      "metadata": {
        "id": "XTE7UQVJ7m3S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "def save_png(name,ep,bs):\n",
        "  date = datetime.datetime.now()\n",
        "  date = date.strftime(\"%y%m%d%H%M%S\")\n",
        "  print(date)\n",
        "  filename = name + '-' + date\n",
        "  if ep != 0:\n",
        "    filename += '-'+str(ep)+'epochs-'+str(bs)+'batch-size.png'\n",
        "  \n",
        "  plt.savefig('/content/gdrive/My Drive/'+filename, bbox_inches='tight')  \n",
        "  print('file probably saved to: ' + filename)\n",
        " # with open('/content/gdrive/My Drive/'+filename , 'w') as f:\n",
        " #   f.write('content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KFmm13sO--ot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Visualize a feature map / activation**"
      ]
    },
    {
      "metadata": {
        "id": "nNlXxMlJlBP3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "\n",
        "# Extracts the outputs of the top 8 layers:\n",
        "layer_outputs = [layer.output for layer in my_model.layers[:6]]\n",
        "# Creates a model that will return these outputs, given the model input:\n",
        "activation_model = models.Model(inputs=my_model.input, outputs=layer_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g96HxbbsrvYU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_tensor = x_train[img_index]\n",
        "# reshape from 3d to 4d\n",
        "img_tensor = img_tensor.reshape(1,28,28,1)\n",
        "activations = activation_model.predict(img_tensor)\n",
        "first_layer_activation = activations[0]\n",
        "print(\"Dimensions in the first activation layer: \" + str(first_layer_activation.shape[3]))\n",
        "\n",
        "if use_matplotlib:\n",
        "  plt.matshow(first_layer_activation[0, :, :, 0], cmap='magma')\n",
        "  save_png('feature-map',0,0)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oMVq_um-uD_x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Evaluate the model\n",
        "//Try the model using only the pretrained downloaded weights\n",
        "Evaluat the model"
      ]
    },
    {
      "metadata": {
        "id": "qGRkbfAbuBG5",
        "colab_type": "code",
        "outputId": "cd4fac3e-dc83-443b-8b46-9288391f9eb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "my_model.load_weights('best.hdf5')\n",
        "predictions = my_model.predict(x_train)\n",
        "print(len(predictions))\n",
        "score = my_model.evaluate(x_test, y_test, verbose=1)\n",
        "print(score)\n",
        "# Show test accuracy\n",
        "print('Loss: ', score[0],', Accuracy:', score[1])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55000\n",
            "10000/10000 [==============================] - 1s 108us/step\n",
            "[0.2176256357550621, 0.9197]\n",
            "Loss:  0.2176256357550621 , Accuracy: 0.9197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hLJmyqsQi1fc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}